{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9VnUcMc0Bsr"
      },
      "source": [
        "# Disentanglement Playground with Pretrained Protein LMs\n",
        "\n",
        "This notebook shows how to:\n",
        "1. Download & encode sequences via ESM and ProtTrans\n",
        "2. Build small latents (β-VAE / FactorVAE) on top of frozen embeddings\n",
        "3. Contrastive heads (SimCLR style)\n",
        "4. Train & inspect disentanglement metrics\n"
      ],
      "id": "B9VnUcMc0Bsr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roQm35wS0Bs2"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "Install dependencies (run once):"
      ],
      "id": "roQm35wS0Bs2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDgd7UFY0Bs4",
        "outputId": "9c24de57-3971-4f02-c1bd-e340964280fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: fair-esm in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.7.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.14.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.20.1)\n",
            "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n"
          ]
        }
      ],
      "source": [
        "!pip install biopython torch torchvision pytorch-lightning transformers fair-esm scikit-learn"
      ],
      "id": "UDgd7UFY0Bs4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVI_-FYl0Bs5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import esm  # if using FAIR’s esm repo"
      ],
      "id": "qVI_-FYl0Bs5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download FASTA-format Datasets"
      ],
      "metadata": {
        "id": "t1hjdcD1VDBh"
      },
      "id": "t1hjdcD1VDBh"
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez, SeqIO\n",
        "Entrez.email = \"pompos002@gmail.com\"\n",
        "handle = Entrez.efetch(db=\"protein\",\n",
        "                       id=[\"P01308\",\"P01308\"],  # UniProt IDs\n",
        "                       rettype=\"fasta\", retmode=\"text\")\n",
        "records = list(SeqIO.parse(handle, \"fasta\"))\n"
      ],
      "metadata": {
        "id": "k0n-UCMJVHC0"
      },
      "id": "k0n-UCMJVHC0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1) load FASTA as HF Dataset\n",
        "hf_ds = load_dataset(\"fasta\",\n",
        "                     data_files={\"train\": \"data/my_proteins.fasta\"},\n",
        "                     split=\"train\")\n",
        "\n",
        "# 2) wrap it so it returns just sequences\n",
        "class SeqDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_dataset):\n",
        "        self.ds = hf_dataset\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "    def __getitem__(self, i):\n",
        "        # hf_ds[i] has 'sequence' & 'description'\n",
        "        return self.ds[i][\"sequence\"]\n",
        "\n",
        "seq_ds = SeqDataset(hf_ds)\n",
        "\n",
        "# 3) plug into your ProteinEmbeddingDataset\n",
        "embed_ds = ProteinEmbeddingDataset(\n",
        "    sequences=seq_ds,               # assume you adapt it to take a list or iterator of seqs\n",
        "    model_name=\"esm2_t33_650M_UR50D\",\n",
        "    aggregation=\"cls\",\n",
        "    max_len=512,\n",
        "    device=\"cuda\"\n",
        ")\n",
        "\n",
        "loader = DataLoader(embed_ds, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zdnYBbNCVr9K",
        "outputId": "3729e274-6a0c-43ca-9e6d-38c5e4b5f89e"
      },
      "id": "zdnYBbNCVr9K",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Couldn't find a dataset script at /content/fasta/fasta.py or any data file in the same directory. Couldn't find 'fasta' on the Hugging Face Hub either: FileNotFoundError: Dataset 'fasta' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-18-2836679460.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 1) load FASTA as HF Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m hf_ds = load_dataset(\"fasta\",\n\u001b[0m\u001b[1;32m      8\u001b[0m                      \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"data/my_proteins.fasta\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                      split=\"train\")\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2112\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2113\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1798\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1799\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1489\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m                     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1492\u001b[0m                         \u001b[0;34mf\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /content/fasta/fasta.py or any data file in the same directory. Couldn't find 'fasta' on the Hugging Face Hub either: FileNotFoundError: Dataset 'fasta' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqqTSq1p0Bs6"
      },
      "source": [
        "## 2. Data & Embedding Extraction\n",
        "\n",
        "Define a simple FASTA dataset and functions to extract embeddings from ESM and ProtTrans."
      ],
      "id": "MqqTSq1p0Bs6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyBEecqQ0Bs7"
      },
      "outputs": [],
      "source": [
        "class FastaDataset(Dataset):\n",
        "    def __init__(self, fasta_path, tokenizer, max_len=512):\n",
        "        # load sequences\n",
        "        from Bio import SeqIO\n",
        "        self.seqs = [str(rec.seq) for rec in SeqIO.parse(fasta_path, \"fasta\")]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.seqs[idx]\n",
        "        enc = self.tokenizer(seq,\n",
        "                             truncation=True,\n",
        "                             padding='max_length',\n",
        "                             max_length=self.max_len,\n",
        "                             return_tensors='pt')\n",
        "        return seq, enc"
      ],
      "id": "UyBEecqQ0Bs7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp7in09o0Bs8",
        "outputId": "4eca9689-43bc-48ab-e964-02f843ff7b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ESM2(\n",
              "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
              "  (layers): ModuleList(\n",
              "    (0-32): 33 x TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (contact_head): ContactPredictionHead(\n",
              "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
              "    (activation): Sigmoid()\n",
              "  )\n",
              "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Load ESM\n",
        "esm_model, esm_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "esm_batch_converter = esm_alphabet.get_batch_converter()\n",
        "esm_model.eval()\n",
        "\n",
        "# Load ProtTrans (ProtT5)\n",
        "# pt_model = AutoModel.from_pretrained(\n",
        "#     \"Rostlab/prot_t5_xl_uniref50\", trust_remote_code=True\n",
        "# )\n",
        "# pt_tokenizer = AutoTokenizer.from_pretrained(\n",
        "#     \"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False\n",
        "# )"
      ],
      "id": "yp7in09o0Bs8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ProteinEmbedding dataset class (model agnostic)"
      ],
      "metadata": {
        "id": "-fb5Q-hlTW3u"
      },
      "id": "-fb5Q-hlTW3u"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class ProteinEmbeddingDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A Dataset that:\n",
        "     - reads sequences from a FASTA\n",
        "     - tokenizes them for either ESM or ProtTrans\n",
        "     - runs the frozen model to get per-residue embeddings\n",
        "     - aggregates them into a fixed-size vector via 'mean', 'cls', or 'max'\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        fasta_path: str,\n",
        "        model_name: str = \"esm2_t33_650M_UR50D\",    # or \"prot_t5_xl_uniref50\"\n",
        "        aggregation: str = \"mean\",                   # one of \"mean\", \"cls\", \"max\"\n",
        "        max_len: int = 512,\n",
        "        device: str = \"cpu\",\n",
        "    ):\n",
        "        from Bio import SeqIO\n",
        "        self.seqs = [str(rec.seq) for rec in SeqIO.parse(fasta_path, \"fasta\")]\n",
        "        self.aggregation = aggregation\n",
        "        self.max_len = max_len\n",
        "        self.device = device\n",
        "\n",
        "        if model_name.startswith(\"esm\"):\n",
        "            import esm\n",
        "            self.model, self.alphabet = getattr(esm.pretrained, model_name)()\n",
        "            self.batch_converter = self.alphabet.get_batch_converter()\n",
        "            self.model.eval().to(device)\n",
        "            self.backend = \"esm\"\n",
        "        else:\n",
        "            from transformers import AutoModel, AutoTokenizer\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
        "            self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "            self.model.eval().to(device)\n",
        "            self.backend = \"prottrans\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.seqs[idx][: self.max_len]  # truncate if too long\n",
        "        if self.backend == \"esm\":\n",
        "            # esm wants a batch of tuples: (name, seq)\n",
        "            batch = [(str(idx), seq)]\n",
        "            _, _, tokens = self.batch_converter(batch)\n",
        "            tokens = tokens.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                out = self.model(tokens, repr_layers=[self.model.num_layers])\n",
        "            # repr_layers returns a dict: layer_index → (batch, L, C)\n",
        "            reps = out[\"representations\"][self.model.num_layers]  # shape (1, L, C)\n",
        "            mask = (tokens != self.alphabet.padding_idx).unsqueeze(-1)  # (1, L, 1)\n",
        "        else:\n",
        "            enc = self.tokenizer(\n",
        "                seq,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.max_len,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                out = self.model(**enc, output_hidden_states=False)\n",
        "            # last_hidden_state: (1, L, C)\n",
        "            reps = out.last_hidden_state\n",
        "            # ProtT5 uses tokenizer.pad_token_id for padding\n",
        "            mask = (enc[\"attention_mask\"].unsqueeze(-1).bool()).to(self.device)\n",
        "\n",
        "        # reps: (1, L, C), mask: (1, L, 1)\n",
        "        reps = reps.squeeze(0)   # → (L, C)\n",
        "        mask = mask.squeeze(0)   # → (L, 1)\n",
        "\n",
        "        if self.aggregation == \"mean\":\n",
        "            summed = (reps * mask).sum(0)                # (C,)\n",
        "            lengths = mask.sum(0).clamp(min=1)           # (C,) broadcastable\n",
        "            emb = summed / lengths\n",
        "        elif self.aggregation == \"max\":\n",
        "            # mask out padded positions by very negative\n",
        "            reps_masked = reps.masked_fill(~mask, -1e9)\n",
        "            emb, _ = reps_masked.max(0)                  # (C,)\n",
        "        elif self.aggregation == \"cls\":\n",
        "            # For ESM, the first token is <cls> (index 0);\n",
        "            # for ProtTrans (like T5), token 0 is <s> which you can treat like CLS\n",
        "            emb = reps[0]\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown aggregation: {self.aggregation}\")\n",
        "\n",
        "        return emb.cpu()\n",
        "\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# --- 1. Wrap the embedding dataset so it yields {'emb': tensor} for Lightning ---\n",
        "class EmbeddingWrapper(Dataset):\n",
        "    def __init__(self, embed_ds):\n",
        "        self.embed_ds = embed_ds\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embed_ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        emb = self.embed_ds[idx]    # emb: torch.Tensor of shape (C,)\n",
        "        return {\"emb\": emb}"
      ],
      "metadata": {
        "id": "-ofYrlV8TVrk"
      },
      "id": "-ofYrlV8TVrk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SopFJ7c80Bs9"
      },
      "outputs": [],
      "source": [
        "def get_esm_embeddings(batch_seqs):\n",
        "    # batch_seqs: List of tuples (name, seq)\n",
        "    _, _, toks = esm_batch_converter(batch_seqs)\n",
        "    with torch.no_grad():\n",
        "        out = esm_model(toks, repr_layers=[33])\n",
        "    return out[\"representations\"][33].mean(1)  # mean over residues\n",
        "\n",
        "def get_pt_embeddings(encodings):\n",
        "    input_ids = encodings[\"input_ids\"].squeeze(0)\n",
        "    attention_mask = encodings[\"attention_mask\"].squeeze(0)\n",
        "    with torch.no_grad():\n",
        "        out = pt_model(\n",
        "            input_ids[None], attention_mask=attention_mask[None]\n",
        "        )\n",
        "    mask = attention_mask[:, None].bool()\n",
        "    emb = (out.last_hidden_state * mask).sum(1) / mask.sum(1)\n",
        "    return emb"
      ],
      "id": "SopFJ7c80Bs9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGWS-44N0Bs-"
      },
      "source": [
        "## 3. β-VAE / FactorVAE Modules\n",
        "\n",
        "Define the VAE architecture and LightningModule wrapper."
      ],
      "id": "vGWS-44N0Bs-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSBNyHqo0BtF"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, z_dim=32, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, z_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, z_dim)\n",
        "        self.fc_dec = nn.Linear(z_dim, hidden_dim)\n",
        "        self.fc_out = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = (0.5 * logvar).exp()\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc_dec(z))\n",
        "        return self.fc_out(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ],
      "id": "KSBNyHqo0BtF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwp_5gZv0BtG"
      },
      "outputs": [],
      "source": [
        "class VAETrainer(pl.LightningModule):\n",
        "    def __init__(self, input_dim, z_dim=32, beta=4.0):\n",
        "        super().__init__()\n",
        "        self.model = VAE(input_dim, z_dim)\n",
        "        self.beta = beta\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch['emb']  # embedding vector\n",
        "        recon, mu, logvar = self.model(x)\n",
        "        recon_loss = F.mse_loss(recon, x, reduction='mean')\n",
        "        kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        loss = recon_loss + self.beta * kl\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
      ],
      "id": "Pwp_5gZv0BtG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z9CpVc80BtH"
      },
      "source": [
        "## 4. SimCLR Head\n",
        "\n",
        "Define the projection head and NT-Xent loss."
      ],
      "id": "-z9CpVc80BtH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpzjFd460BtH"
      },
      "outputs": [],
      "source": [
        "class SimCLRHead(nn.Module):\n",
        "    def __init__(self, emb_dim, proj_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(emb_dim, proj_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
        "    N = z_i.size(0)\n",
        "    z = torch.cat([z_i, z_j], dim=0)\n",
        "    sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)\n",
        "    sim_exp = torch.exp(sim / temperature)\n",
        "    mask = ~torch.eye(2 * N, dtype=bool, device=z.device)\n",
        "    sim_exp = sim_exp.masked_select(mask).view(2 * N, -1)\n",
        "    positive = torch.exp(F.cosine_similarity(z_i, z_j) / temperature)\n",
        "    positives = torch.cat([positive, positive], dim=0)\n",
        "    loss = -torch.log(positives / sim_exp.sum(dim=1))\n",
        "    return loss.mean()"
      ],
      "id": "mpzjFd460BtH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC3ZtQsk0BtI"
      },
      "source": [
        "## 5. Putting It Together\n",
        "\n",
        "Combine embeddings with SimCLR training in a LightningModule."
      ],
      "id": "EC3ZtQsk0BtI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXYXkyjF0BtI"
      },
      "outputs": [],
      "source": [
        "class SimCLRTrainer(pl.LightningModule):\n",
        "    def __init__(self, emb_dim, proj_dim=64, temp=0.5):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Identity()  # embeddings precomputed\n",
        "        self.head = SimCLRHead(emb_dim, proj_dim)\n",
        "        self.temp = temp\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        z1 = self.head(batch['emb1'])\n",
        "        z2 = self.head(batch['emb2'])\n",
        "        loss = nt_xent_loss(z1, z2, self.temp)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=3e-4)"
      ],
      "id": "lXYXkyjF0BtI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brUqswtC0BtI"
      },
      "source": [
        "## 6. Workflow\n",
        "\n",
        "Prepare datasets and train.\n"
      ],
      "id": "brUqswtC0BtI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install numpy==1.26 torch torchvision pytorch-lightning transformers fair-esm biopython datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7E0V3hjfY4tD",
        "outputId": "1ab4d03e-c97f-4d68-bfd5-ce83a2083d91"
      },
      "id": "7E0V3hjfY4tD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26\n",
            "  Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: fair-esm in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.11/dist-packages (1.85)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.7.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.14.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "f453133a958e488db546c413200efa0a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create example FASTA so the path exists\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with open(\"data/my_proteins.fasta\", \"w\") as f:\n",
        "    f.write(\"\"\">protein1\n",
        "MKWVTFISLLFLFSSAYSRGVFRRDTHKSEIAHRFKDLGE\n",
        ">protein2\n",
        "GILGYTEAQVKILDGGSGFYTNLTMATPLKAPIK\n",
        ">protein3\n",
        "MTIQTGLDSTGTTMTVVESKDLKELLEAQQGIQAYSQVGR\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from Bio import SeqIO\n",
        "from datasets import Dataset\n",
        "\n",
        "import esm   # FAIR’s ESM library\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# 1) Read your FASTA and build an HF Dataset\n",
        "fasta_path = \"data/my_proteins.fasta\"\n",
        "records = list(SeqIO.parse(fasta_path, \"fasta\"))\n",
        "ids   = [rec.id  for rec in records]\n",
        "seqs  = [str(rec.seq) for rec in records]\n",
        "hf_ds = Dataset.from_dict({\"id\": ids, \"sequence\": seqs})\n",
        "\n",
        "# 2) Load the frozen ESM model once\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "esm_model, esm_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "esm_model = esm_model.eval().to(device)\n",
        "batch_converter = esm_alphabet.get_batch_converter()\n",
        "pad_idx   = esm_alphabet.padding_idx\n",
        "num_layer = esm_model.num_layers\n",
        "\n",
        "# 3) Compute embeddings via map()\n",
        "def embed_batch(batch):\n",
        "    names = batch[\"id\"]\n",
        "    seqs  = batch[\"sequence\"]\n",
        "    _, _, toks = batch_converter(list(zip(names, seqs)))\n",
        "    toks = toks.to(device)\n",
        "    with torch.no_grad():\n",
        "        out = esm_model(toks, repr_layers=[num_layer])\n",
        "    reps = out[\"representations\"][num_layer]    # (B, L, C)\n",
        "    mask = (toks != pad_idx).unsqueeze(-1)      # (B, L, 1)\n",
        "    summed   = (reps * mask).sum(1)             # (B, C)\n",
        "    lengths  = mask.sum(1).clamp(min=1)         # (B, 1)\n",
        "    emb      = summed / lengths                # (B, C)\n",
        "    return {\"emb\": emb.cpu().numpy().tolist()}\n",
        "\n",
        "hf_emb = hf_ds.map(embed_batch, batched=True, batch_size=16, remove_columns=[\"id\", \"sequence\"])\n",
        "hf_emb.set_format(type=\"torch\", columns=[\"emb\"])\n",
        "\n",
        "# 4) Prepare DataLoader\n",
        "sample = hf_emb[0][\"emb\"]\n",
        "input_dim = sample.shape[0]\n",
        "print(f\"Embedding dim = {input_dim}\")\n",
        "\n",
        "train_loader = DataLoader(hf_emb, batch_size=32, shuffle=True, num_workers=0)\n",
        "\n",
        "# 5) Define VAE Trainer\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, z_dim=32, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, z_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, z_dim)\n",
        "        self.fc_dec = nn.Linear(z_dim, hidden_dim)\n",
        "        self.fc_out = nn.Linear(hidden_dim, input_dim)\n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = (0.5*logvar).exp()\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc_dec(z))\n",
        "        return self.fc_out(h)\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "class VAETrainer(pl.LightningModule):\n",
        "    def __init__(self, input_dim, z_dim=32, beta=4.0):\n",
        "        super().__init__()\n",
        "        self.model = VAE(input_dim, z_dim)\n",
        "        self.beta = beta\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch[\"emb\"]\n",
        "        recon, mu, logvar = self.model(x)\n",
        "        recon_loss = F.mse_loss(recon, x, reduction='mean')\n",
        "        kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        loss = recon_loss + self.beta * kl\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "# 6) Train!\n",
        "vae_module = VAETrainer(input_dim=input_dim, z_dim=32, beta=4.0)\n",
        "trainer = pl.Trainer(max_epochs=5, accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\") # Updated gpus to accelerator\n",
        "trainer.fit(vae_module, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437,
          "referenced_widgets": [
            "fb100f72efb8484d849d89860087bcf6",
            "5d9509dee107447fad903ef218ec6347",
            "536d8a91ba21497f91e7908e481dee6a",
            "24a9b6ff36414f62bd54562a0aae7ae6",
            "5867a753162941a2b7f19b25ee37edb3",
            "91648d311c33415f8e1fff367dd71cc8",
            "60e0f2dd6a704fc8851a4f3d81a180cd",
            "7e7cf2dad8e44b52abc421f0ad528701",
            "0924ef40bf9147dbaeada1c9021e585a",
            "64afd5ced7a3413e83049649bafdc9a7",
            "3f4d76c0180e4c3ab543f4b8c4539dec",
            "b75c9adc86cd4eaab5b23c06dbca59ad",
            "479f317894524510952f1a3522cb2aac",
            "a44be6dd969641f0845c7fc2af15ff66",
            "3af3aa5ff8454f33bbe187de679cf6f7",
            "9f47bfb6654f49808257073f3e79c82a",
            "8725c3a53c4e44abb444b275b4297765",
            "c92bc2db0d874d4d98e0dacdef987d3d",
            "c629e2c0623c464ab94c0eb0e10f025e",
            "698cb2b813e1400d95abe2eb03c75489",
            "3ac086dbc4394631b1c85f59b4bdc956",
            "7a49d2647cd843faa18cd34e382a0989"
          ]
        },
        "id": "Hupr2nenXRis",
        "outputId": "868a0cc0-a27d-489f-9da9-f36806344dcd"
      },
      "id": "Hupr2nenXRis",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb100f72efb8484d849d89860087bcf6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dim = 1280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type | Params | Mode \n",
            "---------------------------------------\n",
            "0 | model | VAE  | 681 K  | train\n",
            "---------------------------------------\n",
            "681 K     Trainable params\n",
            "0         Non-trainable params\n",
            "681 K     Total params\n",
            "2.727     Total estimated model params size (MB)\n",
            "6         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b75c9adc86cd4eaab5b23c06dbca59ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb100f72efb8484d849d89860087bcf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d9509dee107447fad903ef218ec6347",
              "IPY_MODEL_536d8a91ba21497f91e7908e481dee6a",
              "IPY_MODEL_24a9b6ff36414f62bd54562a0aae7ae6"
            ],
            "layout": "IPY_MODEL_5867a753162941a2b7f19b25ee37edb3"
          }
        },
        "5d9509dee107447fad903ef218ec6347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91648d311c33415f8e1fff367dd71cc8",
            "placeholder": "​",
            "style": "IPY_MODEL_60e0f2dd6a704fc8851a4f3d81a180cd",
            "value": "Map: 100%"
          }
        },
        "536d8a91ba21497f91e7908e481dee6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e7cf2dad8e44b52abc421f0ad528701",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0924ef40bf9147dbaeada1c9021e585a",
            "value": 3
          }
        },
        "24a9b6ff36414f62bd54562a0aae7ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64afd5ced7a3413e83049649bafdc9a7",
            "placeholder": "​",
            "style": "IPY_MODEL_3f4d76c0180e4c3ab543f4b8c4539dec",
            "value": " 3/3 [00:00&lt;00:00,  6.33 examples/s]"
          }
        },
        "5867a753162941a2b7f19b25ee37edb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91648d311c33415f8e1fff367dd71cc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60e0f2dd6a704fc8851a4f3d81a180cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e7cf2dad8e44b52abc421f0ad528701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0924ef40bf9147dbaeada1c9021e585a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64afd5ced7a3413e83049649bafdc9a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f4d76c0180e4c3ab543f4b8c4539dec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b75c9adc86cd4eaab5b23c06dbca59ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_479f317894524510952f1a3522cb2aac",
              "IPY_MODEL_a44be6dd969641f0845c7fc2af15ff66",
              "IPY_MODEL_3af3aa5ff8454f33bbe187de679cf6f7"
            ],
            "layout": "IPY_MODEL_9f47bfb6654f49808257073f3e79c82a"
          }
        },
        "479f317894524510952f1a3522cb2aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8725c3a53c4e44abb444b275b4297765",
            "placeholder": "​",
            "style": "IPY_MODEL_c92bc2db0d874d4d98e0dacdef987d3d",
            "value": "Epoch 4: 100%"
          }
        },
        "a44be6dd969641f0845c7fc2af15ff66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c629e2c0623c464ab94c0eb0e10f025e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_698cb2b813e1400d95abe2eb03c75489",
            "value": 1
          }
        },
        "3af3aa5ff8454f33bbe187de679cf6f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ac086dbc4394631b1c85f59b4bdc956",
            "placeholder": "​",
            "style": "IPY_MODEL_7a49d2647cd843faa18cd34e382a0989",
            "value": " 1/1 [00:00&lt;00:00, 25.22it/s, v_num=0]"
          }
        },
        "9f47bfb6654f49808257073f3e79c82a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "8725c3a53c4e44abb444b275b4297765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c92bc2db0d874d4d98e0dacdef987d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c629e2c0623c464ab94c0eb0e10f025e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "698cb2b813e1400d95abe2eb03c75489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ac086dbc4394631b1c85f59b4bdc956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a49d2647cd843faa18cd34e382a0989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}